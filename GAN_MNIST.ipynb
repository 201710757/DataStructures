{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_MNIST",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dudNHLpi9kl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwQlZufeBLRE",
        "colab_type": "code",
        "outputId": "912afbaa-ab48-416a-ed9f-de8c649048d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a-EHHKI9yOx",
        "colab_type": "code",
        "outputId": "ab265de4-dd0a-491d-fdc5-52db6ad37200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEXv7uuD-GRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x = mnist.train.images\n",
        "train_y = mnist.train.labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn7ImvIV-TX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_epochs = 300\n",
        "batch_size = 100\n",
        "learning_rate = 0.0001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHmKjApCACE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator( z , reuse = False ) :\n",
        "    if reuse==False :\n",
        "        with tf.variable_scope(name_or_scope = \"Gen\") as scope :\n",
        "            gw1 = tf.get_variable(name = \"w1\",\n",
        "                                  shape = [128, 256],\n",
        "                                  initializer= tf.random_normal_initializer(mean=0.0, stddev = 0.01))\n",
        "            # quiz : weight 의 초깃값을 random normal 로 주는 이유는 무엇일까요?\n",
        "\n",
        "            gb1 = tf.get_variable(name = \"b1\",\n",
        "                                 shape = [256],\n",
        "                                 initializer = tf.random_normal_initializer(mean=0.0, stddev = 0.01))\n",
        "\n",
        "            gw2 = tf.get_variable(name = \"w2\",\n",
        "                                  shape = [256, 784],\n",
        "                                  initializer= tf.random_normal_initializer(mean=0.0, stddev = 0.01))\n",
        "\n",
        "            gb2 = tf.get_variable(name = \"b2\",\n",
        "                                 shape = [784],\n",
        "                                 initializer = tf.random_normal_initializer(mean=0.0, stddev = 0.01))\n",
        "    else :\n",
        "        with tf.variable_scope(name_or_scope=\"Gen\", reuse = True) as scope :\n",
        "            gw1 = tf.get_variable(name = \"w1\",\n",
        "                                  shape = [128, 256],\n",
        "                                  initializer= tf.random_normal_initializer(mean=0.0, stddev = 0.01))\n",
        "\n",
        "            gb1 = tf.get_variable(name = \"b1\",\n",
        "                                 shape = [256],\n",
        "                                 initializer = tf.random_normal_initializer(mean=0.0, stddev = 0.01))\n",
        "\n",
        "            gw2 = tf.get_variable(name = \"w2\",\n",
        "                                  shape = [256, 784],\n",
        "                                  initializer= tf.random_normal_initializer(mean=0.0, stddev = 0.01))\n",
        "\n",
        "            gb2 = tf.get_variable(name = \"b2\",\n",
        "                                 shape = [784],\n",
        "                                 initializer = tf.random_normal_initializer(mean=0.0, stddev = 0.01))\n",
        "\n",
        "\n",
        "    hidden = tf.nn.relu( tf.matmul(z , gw1) + gb1 )\n",
        "    output = tf.nn.sigmoid( tf.matmul(hidden, gw2) + gb2 )   #출력층에 시그모이드를 쓰는 이유는 무엇일까요?\n",
        "\n",
        "    return output   #[784] 가짜 생성된 이미지"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyz4ta-X-Ytm",
        "colab_type": "code",
        "outputId": "a4178518-1cac-42f1-81f7-fc8af79db04d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "\"\"\"\n",
        "def generator(z, reuse = False):\n",
        "  if reuse == False:\n",
        "    with tf.variable_scope(name_or_scope= \"Gen\") as scope:\n",
        "      gw1 = tf.get_variable(name = \"w1\", shape = [128, 256], initializer = tf.random_normal_initializer(mean = 0.0, stddev = 0.01))\n",
        "      gb1 = tf.get_varialbe(name = \"b1\", shape = [256], initializer = tf.random_normal_initializer(mean = 0.0, stddev = 0.01))\n",
        "      gw2 = tf.get_variable(name = \"w2\", shape = [256, 784], initializer = tf.random_normal_initializer(mean = 0.0, stddev = 0.01))\n",
        "      gb2 = tf.get_varialbe(name = \"b2\", shape = [784], initializer = tf.random_normal_initializer(mean = 0.0, stddev = 0.01))\n",
        "  else:\n",
        "    with tf.variable_scope(name_or_scope = \"Gen\", reuse = True) as scope:\n",
        "      gw1 = tf.get_variable(name = \"w1\", shape = [128, 256], initializer = tf.random_normal_initializer(mean = 0.0, stddev = 0.01))\n",
        "      gb1 = tf.get_varialbe(name = \"b1\", shape = [256], initializer = tf.random_normal_initializer(mean = 0.0, stddev = 0.01))\n",
        "      gw2 = tf.get_variable(name = \"w2\", shape = [256, 784], initializer = tf.random_normal_initializer(mean = 0.0, stddev = 0.01))\n",
        "      gb2 = tf.get_varialbe(name = \"b2\", shape = [784], initializer = tf.random_normal_initializer(mean = 0.0, stddev = 0.01))\n",
        "   \n",
        "   hidden = tf.nn.relu( tf.matmul(z , gw1) + gb1 )\n",
        "   output = tf.nn.sigmoid(tf.matmul(hidden, gw2) + gb2)\n",
        "\n",
        "   return output\n",
        "   \"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef generator(z, reuse = False):\\n  if reuse == False:\\n    with tf.variable_scope(name_or_scope= \"Gen\") as scope:\\n      gw1 = tf.get_variable(name = \"w1\", shape = [128, 256], initializer = tf.random_normal_initializer(mean = 0.0, stddev = 0.01))\\n      gb1 = tf.get_varialbe(name = \"b1\", shape = [256], initializer = tf.random_normal_initializer(mean = 0.0, stddev = 0.01))\\n      gw2 = tf.get_variable(name = \"w2\", shape = [256, 784], initializer = tf.random_normal_initializer(mean = 0.0, stddev = 0.01))\\n      gb2 = tf.get_varialbe(name = \"b2\", shape = [784], initializer = tf.random_normal_initializer(mean = 0.0, stddev = 0.01))\\n  else:\\n    with tf.variable_scope(name_or_scope = \"Gen\", reuse = True) as scope:\\n      gw1 = tf.get_variable(name = \"w1\", shape = [128, 256], initializer = tf.random_normal_initializer(mean = 0.0, stddev = 0.01))\\n      gb1 = tf.get_varialbe(name = \"b1\", shape = [256], initializer = tf.random_normal_initializer(mean = 0.0, stddev = 0.01))\\n      gw2 = tf.get_variable(name = \"w2\", shape = [256, 784], initializer = tf.random_normal_initializer(mean = 0.0, stddev = 0.01))\\n      gb2 = tf.get_varialbe(name = \"b2\", shape = [784], initializer = tf.random_normal_initializer(mean = 0.0, stddev = 0.01))\\n   \\n   hidden = tf.nn.relu( tf.matmul(z , gw1) + gb1 )\\n   output = tf.nn.sigmoid(tf.matmul(hidden, gw2) + gb2)\\n\\n   return output\\n   '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVh9HIt5AWIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator( x , reuse = False) :\n",
        "\n",
        "    if(reuse == False) :\n",
        "        with tf.variable_scope(name_or_scope=\"Dis\") as scope :\n",
        "            dw1 = tf.get_variable(name = \"w1\",\n",
        "                                  shape = [784, 256],\n",
        "                                  initializer = tf.random_normal_initializer(0.0, 0.01) )\n",
        "            db1 = tf.get_variable(name = \"b1\",\n",
        "                                  shape = [256],\n",
        "                                  initializer = tf.random_normal_initializer(0.0, 0.01) )\n",
        "            dw2 = tf.get_variable(name = \"w2\",\n",
        "                                  shape = [256, 1],\n",
        "                                  initializer = tf.random_normal_initializer(0.0, 0.01) )\n",
        "            db2 = tf.get_variable(name = \"b2\",\n",
        "                                  shape = [1],\n",
        "                                  initializer = tf.random_normal_initializer(0.0, 0.01) )\n",
        "    else :\n",
        "        with tf.variable_scope(name_or_scope=\"Dis\", reuse = True) as scope :\n",
        "            dw1 = tf.get_variable(name = \"w1\",\n",
        "                                  shape = [784, 256],\n",
        "                                  initializer = tf.random_normal_initializer(0.0, 0.01) )\n",
        "            db1 = tf.get_variable(name = \"b1\",\n",
        "                                  shape = [256],\n",
        "                                  initializer = tf.random_normal_initializer(0.0, 0.01) )\n",
        "            dw2 = tf.get_variable(name = \"w2\",\n",
        "                                  shape = [256, 1],\n",
        "                                  initializer = tf.random_normal_initializer(0.0, 0.01) )\n",
        "            db2 = tf.get_variable(name = \"b2\",\n",
        "                                  shape = [1],\n",
        "                                  initializer = tf.random_normal_initializer(0.0, 0.01) )\n",
        "\n",
        "    hidden = tf.nn.relu( tf.matmul(x , dw1) + db1 )  #[-, 256]\n",
        "    output = tf.nn.sigmoid( tf.matmul(hidden, dw2)  + db2 )   #[-, 1]  진품인지(1) 가품인지(0)의 label 결과값\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyTbPPLFAaz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_noise(batch_size) :\n",
        "    return np.random.normal(size=[batch_size , 128])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGRH9Oz3Acjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g = tf.Graph()\n",
        "\n",
        "with g.as_default() :\n",
        "\n",
        "    ######################################################\n",
        "    # 1 .Feedable part  :: 그래프에서 유일하게 데이터가 유입될 수 있는 장소\n",
        "    ######################################################\n",
        "\n",
        "    X = tf.placeholder(tf.float32, [None, 784]) #GAN 은 autoencoder 와 마찬가지로 unsupervised learning 이므로 y(label)을 사용하지 않습니다.\n",
        "\n",
        "    Z = tf.placeholder(tf.float32, [None, 128]) #Z 는 생성기의 입력값으로 사용될 노이즈 입니다.\n",
        "\n",
        "\n",
        "    ################################\n",
        "    # 2. generator 와 discriminator 의 사용\n",
        "    ##################################\n",
        "\n",
        "\n",
        "    fake_x = generator(Z)\n",
        "\n",
        "    result_of_fake = discriminator(fake_x)\n",
        "    result_of_real = discriminator(X , True)\n",
        "\n",
        "\n",
        "    ################################\n",
        "    # 3. Loss( 성취도평가 ) : g_loss 와 d_loss\n",
        "\n",
        "    # g_loss & d_loss 모두 높을 수록 좋다.\n",
        "    # g_loss : 얼마나 fake_x 가 진짜같은가\n",
        "    # d_loss : 얼마나 discriminator 가 정확한가\n",
        "\n",
        "    # 두 수치를 모두 높이도록 train 하면 생성기와 분류기의 성능이 모두 올라간다.\n",
        "    ################################\n",
        "\n",
        "    g_loss = tf.reduce_mean( tf.log(result_of_fake) )\n",
        "    d_loss = tf.reduce_mean( tf.log(result_of_real) + tf.log(1 - result_of_fake) )\n",
        "\n",
        "\n",
        "    ################################\n",
        "    # 4. Train : Maximizing g_loss & d_loss\n",
        "    ################################\n",
        "\n",
        "    t_vars = tf.trainable_variables() # return list\n",
        "\n",
        "    g_vars = [var for var in t_vars if \"Gen\" in var.name]\n",
        "    d_vars = [var for var in t_vars if \"Dis\" in var.name]\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "    g_train = optimizer.minimize(-g_loss, var_list= g_vars)\n",
        "    d_train = optimizer.minimize(-d_loss, var_list = d_vars)    \n",
        "\n",
        "    # g_loss & d_loss 를 최대화 시켜야하는데 minimize 함수밖에 없기 때문에 - 음수부호 붙인다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvsf3xV3Ae7s",
        "colab_type": "code",
        "outputId": "309c4fde-1cd1-42b0-af1f-3c6f78fecc28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "with tf.Session(graph = g) as sess :\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    total_batchs = int(train_x.shape[0] / batch_size)\n",
        "\n",
        "    for epoch in range(total_epochs) :\n",
        "\n",
        "        for batch in range(total_batchs) :\n",
        "            batch_x = train_x[batch * batch_size : (batch+1) * batch_size]  # [batch_size , 784]\n",
        "            batch_y = train_y[batch * batch_size : (batch+1) * batch_size]  # [batch_size,]\n",
        "            noise = random_noise(batch_size)  # [batch_size, 128]\n",
        "\n",
        "            sess.run(g_train , feed_dict = {Z : noise})\n",
        "            sess.run(d_train, feed_dict = {X : batch_x , Z : noise})\n",
        "\n",
        "            gl, dl = sess.run([g_loss, d_loss], feed_dict = {X : batch_x , Z : noise})\n",
        "\n",
        "\n",
        "        #매 20 epoch 마다 학습된 성능을 중간점검  (실제론 더 자주하시는 것이 좋습니다. )\n",
        "        if (epoch+1) % 20 == 0 or epoch == 1  :\n",
        "            print(\"=======Epoch : \", epoch , \" =======================================\")\n",
        "            print(\"Genorator : \" ,gl )\n",
        "            print(\"Discriminator : \" ,dl )\n",
        "            print(\"Training...\")\n",
        "\n",
        "\n",
        "        #10개의 epoch 마다 Generator 가 만들어내는 실제 결과물을 얻어보며 , 성능 발전을 시각적으로 확인\n",
        "\n",
        "        if epoch == 0 or (epoch + 1) % 10 == 0  :\n",
        "            sample_noise = random_noise(10)\n",
        "\n",
        "            generated = sess.run(fake_x , feed_dict = { Z : sample_noise})\n",
        "\n",
        "            fig, ax = plt.subplots(1, 10, figsize=(10, 1))\n",
        "            for i in range(10) :\n",
        "                ax[i].set_axis_off()\n",
        "                ax[i].imshow( np.reshape( generated[i], (28, 28)) )\n",
        "\n",
        "            plt.savefig('/content/gdrive/My Drive/GAN_TEST/{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')\n",
        "            plt.close(fig)\n",
        "\n",
        "\n",
        "    print('Success!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=======Epoch :  1  =======================================\n",
            "Genorator :  -1.6297112\n",
            "Discriminator :  -0.46166104\n",
            "Training...\n",
            "=======Epoch :  19  =======================================\n",
            "Genorator :  -2.6756463\n",
            "Discriminator :  -0.13664812\n",
            "Training...\n",
            "=======Epoch :  39  =======================================\n",
            "Genorator :  -2.5911922\n",
            "Discriminator :  -0.33158073\n",
            "Training...\n",
            "=======Epoch :  59  =======================================\n",
            "Genorator :  -2.1111884\n",
            "Discriminator :  -0.41886467\n",
            "Training...\n",
            "=======Epoch :  79  =======================================\n",
            "Genorator :  -2.8565311\n",
            "Discriminator :  -0.29706094\n",
            "Training...\n",
            "=======Epoch :  99  =======================================\n",
            "Genorator :  -2.874328\n",
            "Discriminator :  -0.39537212\n",
            "Training...\n",
            "=======Epoch :  119  =======================================\n",
            "Genorator :  -2.2743475\n",
            "Discriminator :  -0.3707071\n",
            "Training...\n",
            "=======Epoch :  139  =======================================\n",
            "Genorator :  -2.3623648\n",
            "Discriminator :  -0.38127664\n",
            "Training...\n",
            "=======Epoch :  159  =======================================\n",
            "Genorator :  -2.5500393\n",
            "Discriminator :  -0.2664114\n",
            "Training...\n",
            "=======Epoch :  179  =======================================\n",
            "Genorator :  -2.4182916\n",
            "Discriminator :  -0.33762878\n",
            "Training...\n",
            "=======Epoch :  199  =======================================\n",
            "Genorator :  -1.7972608\n",
            "Discriminator :  -0.56636137\n",
            "Training...\n",
            "=======Epoch :  219  =======================================\n",
            "Genorator :  -2.4704201\n",
            "Discriminator :  -0.3059366\n",
            "Training...\n",
            "=======Epoch :  239  =======================================\n",
            "Genorator :  -2.531087\n",
            "Discriminator :  -0.26617777\n",
            "Training...\n",
            "=======Epoch :  259  =======================================\n",
            "Genorator :  -2.720458\n",
            "Discriminator :  -0.21687382\n",
            "Training...\n",
            "=======Epoch :  279  =======================================\n",
            "Genorator :  -2.608189\n",
            "Discriminator :  -0.2843261\n",
            "Training...\n",
            "=======Epoch :  299  =======================================\n",
            "Genorator :  -3.1556056\n",
            "Discriminator :  -0.2293544\n",
            "Training...\n",
            "Success!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}